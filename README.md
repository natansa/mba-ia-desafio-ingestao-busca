# ğŸ¤– Chat RAG - Sistema de Perguntas e Respostas sobre Documentos PDF

> **Desafio MBA Engenharia de Software com IA - Full Cycle**

Sistema de chat inteligente baseado em **RAG (Retrieval-Augmented Generation)** que permite fazer perguntas sobre documentos PDF usando modelos de IA da **OpenAI** ou **Google Gemini**.

---

## ğŸ“‹ Ãndice

- [Sobre o Projeto](#-sobre-o-projeto)
- [Arquitetura RAG](#-arquitetura-rag)
- [Funcionalidades](#-funcionalidades)
- [Tecnologias Utilizadas](#-tecnologias-utilizadas)
- [PrÃ©-requisitos](#-prÃ©-requisitos)
- [Estrutura do Projeto](#-estrutura-do-projeto)
- [InstalaÃ§Ã£o](#-instalaÃ§Ã£o)
- [ConfiguraÃ§Ã£o](#-configuraÃ§Ã£o)
- [Como Usar](#-como-usar)
- [Modelos Suportados](#-modelos-suportados)
- [Troubleshooting](#-troubleshooting)
- [ConsideraÃ§Ãµes Importantes](#-consideraÃ§Ãµes-importantes)

---

## ğŸ¯ Sobre o Projeto

Este projeto implementa um sistema completo de **RAG (Retrieval-Augmented Generation)** que:

1. **Processa documentos PDF** dividindo-os em chunks menores
2. **Gera embeddings vetoriais** usando modelos da OpenAI ou Google Gemini
3. **Armazena vetores** em um banco PostgreSQL com extensÃ£o pgvector
4. **Busca por similaridade semÃ¢ntica** para encontrar trechos relevantes
5. **Gera respostas contextualizadas** usando LLMs (Large Language Models)

O sistema garante que as respostas sejam baseadas **exclusivamente no conteÃºdo do documento**, evitando alucinaÃ§Ãµes e informaÃ§Ãµes externas.

---

## ğŸ—ï¸ Arquitetura RAG

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  document.pdf   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. INGESTÃƒO (ingest.py)                                â”‚
â”‚  - Carrega o PDF                                        â”‚
â”‚  - Divide em chunks (1000 chars, overlap 150)           â”‚
â”‚  - Gera embeddings (OpenAI ou Gemini)                   â”‚
â”‚  - Armazena no PostgreSQL com pgvector                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PostgreSQL + pgvector                                  â”‚
â”‚  Collection: document_collection                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. BUSCA (search.py)                                   â”‚
â”‚  - Recebe pergunta do usuÃ¡rio                           â”‚
â”‚  - Converte pergunta em embedding                       â”‚
â”‚  - Busca top 10 chunks mais similares                   â”‚
â”‚  - Monta contexto com os chunks encontrados             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. GERAÃ‡ÃƒO DE RESPOSTA                                 â”‚
â”‚  - Preenche template de prompt com contexto + pergunta  â”‚
â”‚  - Envia para LLM (gpt-5-nano ou gemini-2.5-flash-lite) â”‚
â”‚  - Retorna resposta baseada apenas no contexto          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. CHAT (chat.py)                                      â”‚
â”‚  - Interface interativa para o usuÃ¡rio                  â”‚
â”‚  - Loop de perguntas e respostas                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âœ¨ Funcionalidades

âœ… **Suporte a mÃºltiplos modelos**: OpenAI (gpt-5-nano) e Google Gemini (gemini-2.5-flash-lite)  
âœ… **Processamento inteligente de PDF**: DivisÃ£o em chunks com overlap para manter contexto  
âœ… **Busca semÃ¢ntica avanÃ§ada**: Utiliza embeddings vetoriais para encontrar informaÃ§Ãµes relevantes  
âœ… **Respostas contextualizadas**: LLM responde apenas com base no documento fornecido  
âœ… **Interface de chat interativa**: ConversaÃ§Ã£o contÃ­nua com o sistema  
âœ… **Processamento sob demanda**: OpÃ§Ã£o de processar o PDF direto no chat  
âœ… **ValidaÃ§Ã£o rigorosa**: Previne alucinaÃ§Ãµes e respostas fora do contexto  
âœ… **Armazenamento persistente**: NÃ£o precisa reprocessar o PDF a cada execuÃ§Ã£o  

---

## ğŸ› ï¸ Tecnologias Utilizadas

| Tecnologia | VersÃ£o | Finalidade |
|------------|--------|------------|
| Python | 3.9+ | Linguagem principal |
| LangChain | 0.3.27 | Framework RAG |
| PostgreSQL | 17 | Banco de dados |
| pgvector | 0.7.0 | ExtensÃ£o de vetores |
| OpenAI API | 1.102.0 | Embeddings e LLM |
| Google Gemini API | 2.1.9 | Embeddings e LLM |
| Docker | Latest | ContainerizaÃ§Ã£o do banco |
| python-dotenv | 1.1.1 | Gerenciamento de variÃ¡veis de ambiente |

---

## ğŸ“¦ PrÃ©-requisitos

Antes de comeÃ§ar, certifique-se de ter instalado:

- **Python 3.9+** ([Download](https://www.python.org/downloads/))
- **Docker & Docker Compose** ([Download](https://www.docker.com/products/docker-desktop/))
- **Git** ([Download](https://git-scm.com/downloads))
- **Chave de API OpenAI** ([Obter chave](https://platform.openai.com/api-keys))
- **Chave de API Google Gemini** ([Obter chave](https://aistudio.google.com/app/apikey))

---

## ğŸ“ Estrutura do Projeto

```
mba-ia-desafio-ingestao-busca/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingest.py       # Script de processamento e ingestÃ£o do PDF
â”‚   â”œâ”€â”€ search.py       # LÃ³gica de busca e geraÃ§Ã£o de respostas
â”‚   â””â”€â”€ chat.py         # Interface de chat interativo
â”‚
â”œâ”€â”€ venv/               # Ambiente virtual Python (nÃ£o versionado)
â”œâ”€â”€ document.pdf        # Documento a ser processado
â”œâ”€â”€ docker-compose.yml  # ConfiguraÃ§Ã£o do PostgreSQL + pgvector
â”œâ”€â”€ requirements.txt    # DependÃªncias Python
â”œâ”€â”€ .env.example        # Exemplo de configuraÃ§Ã£o
â”œâ”€â”€ .env                # Suas credenciais (nÃ£o versionado)
â”œâ”€â”€ .gitignore          # Arquivos ignorados pelo Git
â””â”€â”€ README.md           # Este arquivo
```

---

## ğŸš€ InstalaÃ§Ã£o

### 1ï¸âƒ£ Clone o repositÃ³rio

```bash
git clone https://github.com/natansa/mba-ia-desafio-ingestao-busca
cd mba-ia-desafio-ingestao-busca
```

### 2ï¸âƒ£ Crie e ative o ambiente virtual

**Windows (Git Bash):**
```bash
python -m venv venv
source venv/Scripts/activate
```

**Windows (PowerShell):**
```powershell
python -m venv venv
venv\Scripts\Activate.ps1
```

**Windows (CMD):**
```cmd
python -m venv venv
venv\Scripts\activate.bat
```

**Linux/Mac:**
```bash
python3 -m venv venv
source venv/bin/activate
```

### 3ï¸âƒ£ Instale as dependÃªncias

```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Suba o banco de dados PostgreSQL

```bash
docker-compose up -d
```

**Verificar se estÃ¡ rodando:**
```bash
docker ps
```

VocÃª deve ver o container `postgres_rag` em execuÃ§Ã£o.

---

## âš™ï¸ ConfiguraÃ§Ã£o

### 1ï¸âƒ£ Crie o arquivo `.env`

Copie o arquivo de exemplo:

```bash
cp .env.example .env
```

### 2ï¸âƒ£ Configure as variÃ¡veis de ambiente

Edite o arquivo `.env` e adicione suas credenciais:

```env
# =============================================================================
# BANCO DE DADOS
# =============================================================================
DATABASE_URL=postgresql://postgres:postgres@localhost:5432/rag
PG_VECTOR_COLLECTION_NAME=document_collection

# =============================================================================
# DOCUMENTO PDF
# =============================================================================
PDF_PATH=../document.pdf

# =============================================================================
# OPENAI
# =============================================================================
OPENAI_API_KEY=sk-proj-xxxxxxxxxxxxxxxxxxxxxxxxxxxxx
OPENAI_EMBEDDING_MODEL=text-embedding-3-small
OPENAI_LLM_MODEL=gpt-5-nano

# =============================================================================
# GOOGLE GEMINI
# =============================================================================
GOOGLE_API_KEY=AIzaSyxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
GOOGLE_EMBEDDING_MODEL=models/embedding-001
GOOGLE_LLM_MODEL=gemini-2.5-flash-lite
```

### 3ï¸âƒ£ Coloque seu documento PDF

Certifique-se de que o arquivo `document.pdf` estÃ¡ na raiz do projeto (ou ajuste o caminho no `.env`).

---

## ğŸ’¬ Como Usar

### OpÃ§Ã£o 1: Processamento Integrado (Recomendado)

Execute o chat e escolha processar o PDF quando solicitado:

```bash
python src/chat.py
```

**Fluxo interativo:**

```
============================================================
BEM-VINDO AO CHAT RAG
============================================================

*ATENÃ‡ÃƒO: caso o documento jÃ¡ tenha sido processado por outro modelo, 
utilize o mesmo modelo para continuar a conversa.*

Deseja utilizar OPENAI ou GEMINI? openai

*ATENÃ‡ÃƒO: caso o documento jÃ¡ tenha sido processado por outro modelo, 
serÃ¡ necessÃ¡rio processar o documento novamente.*

Deseja processar o arquivo document.pdf antes de iniciar? (sim/nÃ£o): sim

[PROCESSANDO O PDF...]
[SUCESSO] Documento processado e armazenado com sucesso!

Digite 'sair' ou 'exit' para encerrar o chat.

[FAÃ‡A SUA PERGUNTA]: Qual Ã© o tema principal do documento?

[PROCESSANDO...]

[RESPOSTA]: O documento trata sobre... [resposta baseada no PDF]

[FAÃ‡A SUA PERGUNTA]: 
```

### OpÃ§Ã£o 2: Processamento Manual

Processe o PDF primeiro (necessÃ¡rio passar o modelo como argumento):

```bash
python src/ingest.py <openai|gemini>
```
**Somente OPENAI:**
```bash
python src/ingest.py openai
```
**Somente GEMINI:**
```bash
python src/ingest.py gemini
```

Depois inicie o chat:

```bash
python src/chat.py
```

### Comandos disponÃ­veis no chat:

- Digite sua pergunta e pressione **Enter**
- Digite `sair`, `exit`, `quit` ou pressione **Enter** vazio para encerrar
- Use **Ctrl+C** para interromper a qualquer momento

---

## ğŸ¤– Modelos Suportados

### OpenAI

**Embeddings:**
- `text-embedding-3-small`

**LLMs:**
- `gpt-5-nano`

### Google Gemini

**Embeddings:**
- `models/embedding-001`

**LLMs:**
- `gemini-2.5-flash-lite`

---

## ğŸ”§ Troubleshooting

### âŒ Erro: `FileNotFoundError: File ...document.pdf not found`

**SoluÃ§Ã£o:** Verifique se:
- O arquivo `document.pdf` estÃ¡ na raiz do projeto
- O caminho no `.env` estÃ¡ correto: `PDF_PATH=../document.pdf`

### âŒ Erro: `Environment variable X is not set`

**SoluÃ§Ã£o:** Verifique se o arquivo `.env` existe e contÃ©m todas as variÃ¡veis necessÃ¡rias.

### âŒ Erro: `Connection refused` ou `Failed to connect to database`

**SoluÃ§Ã£o:** 
```bash
# Verifique se o Docker estÃ¡ rodando
docker ps

# Se nÃ£o estiver, suba o banco novamente
docker-compose up -d

# Verifique os logs do container
docker logs postgres_rag
```

### âŒ Erro: `Invalid API key` ou `Authentication failed`

**SoluÃ§Ã£o:** Verifique se suas chaves de API no `.env` estÃ£o corretas:
- OpenAI: `OPENAI_API_KEY=sk-proj-...`
- Gemini: `GOOGLE_API_KEY=AIzaSy...`

### âŒ Erro: `Invalid LLM model`

**SoluÃ§Ã£o:** Certifique-se de digitar exatamente `openai` ou `gemini` (minÃºsculas) quando solicitado.

### âŒ Banco de dados corrompido ou duplicaÃ§Ã£o de documentos

**SoluÃ§Ã£o:** Recrie o banco do zero:
```bash
# Pare e remova os containers
docker-compose down -v

# Suba novamente
docker-compose up -d

# Reprocesse o PDF
python src/chat.py
```

---

## âš ï¸ ConsideraÃ§Ãµes Importantes

### ğŸ” SeguranÃ§a
- **NUNCA** compartilhe suas chaves de API publicamente
- **NUNCA** faÃ§a commit do arquivo `.env`
- Use o `.env.example` como referÃªncia, mas mantenha suas credenciais privadas

### ğŸ’° Custos
- OpenAI e Gemini cobram por uso de API
- Embeddings sÃ£o baratos
- LLMs tÃªm custos variados
- Monitore seu uso nos dashboards das plataformas

### ğŸ”„ ConsistÃªncia de Modelos
- **IMPORTANTE**: Use o mesmo modelo de embedding que foi usado para processar o documento
- Se processar com OpenAI, continue usando OpenAI
- Se processar com Gemini, continue usando Gemini
- Misturar modelos resultarÃ¡ em erro

### ğŸ“Š Performance
- **Chunk size**: 1000 caracteres (ajustÃ¡vel em `ingest.py`)
- **Chunk overlap**: 150 caracteres (mantÃ©m contexto entre chunks)
- **Top K**: 10 chunks mais relevantes (ajustÃ¡vel em `search.py`)
- **Temperature**: 0.1 (respostas mais determinÃ­sticas)

### ğŸ“ LimitaÃ§Ãµes
- Funciona apenas com arquivos PDF
- MÃ¡ximo de tokens por consulta depende do modelo escolhido
- Respostas limitadas ao conteÃºdo do documento processado

---

## ğŸ“š ReferÃªncias

- [LangChain Documentation](https://python.langchain.com/)
- [OpenAI API Documentation](https://platform.openai.com/docs)
- [Google Gemini API Documentation](https://ai.google.dev/docs)
- [pgvector Documentation](https://github.com/pgvector/pgvector)

---

## ğŸ‘¨â€ğŸ’» Autor

**https://github.com/natansa**

---

## ğŸ“„ LicenÃ§a

Este projeto foi desenvolvido para fins educacionais.

---

**DÃºvidas?** Revise o [Troubleshooting](#-troubleshooting) ou consulte os logs de erro para mais informaÃ§Ãµes.
